# ğŸ”¢ Handwritten Digit Recognition Using Machine Learning

This project focuses on recognizing handwritten digits (**0â€“9**) from **8x8 grayscale images** using **Machine Learning algorithms**.  
Automatic digit recognition is widely applied in:  
- ğŸ“¬ Postal automation  
- ğŸ¦ Bank check processing  
- ğŸ” Security systems (e.g., CAPTCHA)  

The task is framed as a **multi-class classification problem** where the model predicts one of ten digit classes.

---

## ğŸ“Œ Table of Contents
- [Dataset Overview](#-dataset-overview)
- [Data Exploration](#-data-exploration)
- [Preprocessing](#-preprocessing)
- [Algorithms Used](#-algorithms-used)
- [Model Training Setup](#-model-training-setup)
- [Evaluation Metrics](#-evaluation-metrics)
- [Results](#-results)
- [Confusion Matrix Insights](#-confusion-matrix-insights)
- [Challenges](#-challenges)
- [Conclusion](#-conclusion)
- [Future Improvements](#-future-improvements)

---

## ğŸ“‚ Dataset Overview
- **Samples**: 1,797 handwritten digits  
- **Features**: 64 (pixel values of 8x8 images)  
- **Labels**: Digits `0` to `9`  
- **Pixel Values**: Range from `0` (white) to `16` (black)  
- **Balance**: Dataset is evenly distributed across digit classes  
- **Clean Data**: No missing values  

---

## ğŸ” Data Exploration
- Pixel intensity distributions visualized for clarity.  
- Dataset is **balanced** and suitable for supervised learning.  
- Strong **visual patterns** exist, allowing ML algorithms to differentiate digits.  

---

## âš™ï¸ Preprocessing
Steps applied before training:  
- âœ¨ **Feature Scaling** using `StandardScaler` to normalize pixel values.  
- âœ‚ï¸ **Train-Test Split** â†’ 80% training, 20% testing for generalization.  

---

## ğŸ¤– Algorithms Used
- **Logistic Regression** â†’ baseline linear classifier  
- **K-Nearest Neighbors (KNN)** â†’ similarity-based classification  
- **Naive Bayes** â†’ probabilistic approach with independence assumption  
- **Decision Tree** â†’ splits based on feature thresholds  
- **Support Vector Machine (SVM)** â†’ linear kernel for class separation  

---

## ğŸ›  Model Training Setup
- **Logistic Regression** â†’ `max_iter=1000` for convergence  
- **KNN** â†’ 5 nearest neighbors  
- **SVM** â†’ linear kernel (simple, effective)  
- **Decision Tree / Naive Bayes** â†’ default parameters  

All models were trained on **80% training data** and evaluated on **20% unseen test data**.

---

## ğŸ“Š Evaluation Metrics
- **Accuracy** â†’ percentage of correct predictions  
- **Precision** â†’ correctness of predicted labels  
- **Recall** â†’ ability to capture actual digits  
- **F1 Score** â†’ balance of precision & recall  

---

## ğŸ“ˆ Results

| Model                | Accuracy | Precision | Recall | F1 Score |
|-----------------------|----------|-----------|--------|----------|
| **KNN**              | **0.975** | **0.976** | **0.977** | **0.976** |
| **SVM (Linear)**     | **0.975** | 0.975     | 0.975  | 0.975    |
| Logistic Regression  | 0.972    | 0.974     | 0.974  | 0.974    |
| Decision Tree        | 0.842    | 0.846     | 0.837  | 0.840    |
| Naive Bayes          | 0.767    | 0.817     | 0.770  | 0.760    |

ğŸ† **Top Performers** â†’ **KNN** & **SVM** (both ~97.5% accuracy)  
âš¡ Logistic Regression is a strong, fast baseline  
âš ï¸ Decision Tree and Naive Bayes underperform  

---

## ğŸ”¢ Confusion Matrix Insights
- **Naive Bayes** â†’ frequently misclassifies `5` as `3` or `8`  
- **Decision Tree** â†’ random misclassifications due to overfitting  

---

## âš ï¸ Challenges
- ğŸ“‰ **Small dataset size** (1,797 samples) â†’ risk of overfitting  
- ğŸ‘€ **Visually similar digits** (e.g., `3`, `5`, `8`) cause confusion across models  

---

## âœ… Conclusion
- **KNN & SVM** tied as best models (~97.5% accuracy)  
- **Logistic Regression** â†’ efficient, high-performing baseline  
- **Naive Bayes** struggles due to unrealistic independence assumption  
- **Decision Tree** suffers from **generalization issues**  

---

## ğŸ”® Future Improvements
- ğŸ”§ Use **SVM with RBF kernel** for non-linear separation  
- ğŸ“Š Apply **k-fold cross-validation** for robust generalization  
- ğŸ“‰ Implement **PCA** for dimensionality reduction  
- ğŸŒ² Explore **ensemble models** (Random Forest, Voting Classifier)  
